---
title: "Wine Data Analysis"
author: "Stefany Ayala"
output:
  html_document:
    pandoc_args: ["-V", "geometry:margin=2cm"]
---

## Introduction
This report analyzes a wine dataset using decision trees to classify wine quality. The analysis explores both "information" and "gini" as splitting criteria, evaluates model accuracy, and derives insights from pruned and unpruned trees.

## Dataset Description
The dataset contains attributes of wine samples and their quality ratings. Key variables include:

- **fixed acidity**: Level of fixed acids.
- **volatile acidity**: Level of volatile acids.
- **alcohol**: Alcohol content (%).

Data source: [Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)

## Load and Prepare Data
```{r}
# Load necessary packages
library(rpart)
library(rpart.plot)
library(knitr)
library(kableExtra)

# Load the dataset
url <- "https://drive.google.com/uc?id=1X2q3rh6fzO4ShyQVsfmbQxH79b2g5Fk3&export=download"
wineData <- read.csv(url)

# Convert quality to a factor for classification
wineData$quality <- as.factor(wineData$quality)

# Check structure of the data
str(wineData)
```

## Split Data into Training and Testing Sets
```{r}
# Set seed for reproducibility
set.seed(1234)

# Split data: 70% training, 30% testing
index <- sample(2, nrow(wineData), replace = TRUE, prob = c(0.7, 0.3))
TrainData <- wineData[index == 1, ]
TestData <- wineData[index == 2, ]
```

## Build Decision Tree (Information Criterion)
```{r}
# Build the decision tree using information criterion
wine_rpart <- rpart(quality ~ ., data = TrainData,
                   method = "class", parms = list(split = "information"),
                   control = rpart.control(xval = 5, cp = 0))

# Accuracy on training data
train_conf_mat <- table(predict(wine_rpart, type = 'class'), TrainData$quality,
      dnn=c('Predictec', 'Actual'))
train_conf_mat
train_accuracy_info <- sum(diag(train_conf_mat)) / sum(train_conf_mat)
train_accuracy_info

# Accuracy on testing data
test_conf_mat <- table(predict(wine_rpart, newdata = TestData, type = 'class'), TestData$quality,
      dnn=c('Predictec', 'Actual'))
test_conf_mat
test_accuracy_info <- sum(diag(test_conf_mat)) / sum(test_conf_mat)
test_accuracy_info
```

## Prune Decision Tree (Information Criterion)
```{r}
# Prune the tree using cp that minimizes xerror
opt <- which.min(wine_rpart$cptable[, 'xerror'])
cp <- wine_rpart$cptable[opt, 'CP']
tree_prune_info <- prune(wine_rpart, cp = cp)

# Accuracy on testing data (pruned tree)
pruned_test_conf_mat <- table(predict(tree_prune_info, newdata = TestData, type = 'class'), TestData$quality,
      dnn=c('Predictec', 'Actual'))
pruned_test_conf_mat
pruned_test_accuracy_info <- sum(diag(pruned_test_conf_mat)) / sum(pruned_test_conf_mat)
pruned_test_accuracy_info
```

## Build Decision Tree (Gini Criterion)
```{r}
# Build the decision tree using gini criterion
wine_rpart_gini <- rpart(quality ~ ., data = TrainData,
                         method = "class", parms = list(split = "gini"),
                         control = rpart.control(xval = 5, cp = 0))

# Accuracy on training data
train_conf_mat_gini <- table(predict(wine_rpart_gini, type = 'class'), TrainData$quality,
      dnn=c('Predictec', 'Actual'))
train_conf_mat_gini
train_accuracy_gini <- sum(diag(train_conf_mat_gini)) / sum(train_conf_mat_gini)
train_accuracy_gini

# Accuracy on testing data
test_conf_mat_gini <- table(predict(wine_rpart_gini, newdata = TestData, type = 'class'), TestData$quality,
      dnn=c('Predictec', 'Actual'))
test_conf_mat_gini
test_accuracy_gini <- sum(diag(test_conf_mat_gini)) / sum(test_conf_mat_gini)
test_accuracy_gini
```

## Prune Decision Tree (Gini Criterion)
```{r}
# Prune the tree using cp that minimizes xerror
opt_gini <- which.min(wine_rpart_gini$cptable[, 'xerror'])
cp_gini <- wine_rpart_gini$cptable[opt_gini, 'CP']
tree_prune_gini <- prune(wine_rpart_gini, cp = cp_gini)

# Accuracy on testing data (pruned tree)
pruned_test_conf_mat_gini <- table(predict(tree_prune_gini, newdata = TestData, type = 'class'), TestData$quality,
      dnn=c('Predictec', 'Actual'))
pruned_test_conf_mat_gini
pruned_test_accuracy_gini <- sum(diag(pruned_test_conf_mat_gini)) / sum(pruned_test_conf_mat_gini)
pruned_test_accuracy_gini
```

## Compare Models
```{r}
# Create a comparison table
accuracy_df <- data.frame(
  Model = c("Unpruned (Info)", "Pruned (Info)", "Unpruned (Gini)", "Pruned (Gini)"),
  Accuracy = c(test_accuracy_info, pruned_test_accuracy_info, test_accuracy_gini, pruned_test_accuracy_gini)
)

# Display table
kable(accuracy_df, format = "html", caption = "Model Accuracy Comparison") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## Visualize Decision Trees
```{r}
# Unpruned Decision Tree (Information Criterion)
rpart.plot(wine_rpart,
           type = 3,               # Type 3 includes rules at the nodes
           extra = 104,            # Displays class probabilities and percentages
           under = TRUE,           # Places text under the nodes
           fallen.leaves = TRUE,   # Positions leaves at the bottom of the tree
           tweak = 1.2,            # Adjusts the size of text
           main = "Unpruned Decision Tree (Information Criterion)")

# Pruned Decision Tree (Information Criterion)
rpart.plot(tree_prune_info,
           type = 3,               # Type 3 includes rules at the nodes
           extra = 104,            # Displays class probabilities and percentages
           under = TRUE,           # Places text under the nodes
           fallen.leaves = TRUE,   # Positions leaves at the bottom of the tree
           tweak = 1.2,            # Adjusts the size of text
           main = "Pruned Decision Tree (Information Criterion)")

# Unpruned Decision Tree (Gini Criterion)
rpart.plot(wine_rpart_gini,
           type = 3,               # Type 3 includes rules at the nodes
           extra = 104,            # Displays class probabilities and percentages
           under = TRUE,           # Places text under the nodes
           fallen.leaves = TRUE,   # Positions leaves at the bottom of the tree
           tweak = 1.2,            # Adjusts the size of text
           main = "Unpruned Decision Tree (Gini Criterion)")

# Pruned Decision Tree (Gini Criterion)
rpart.plot(tree_prune_gini,
           type = 3,               # Type 3 includes rules at the nodes
           extra = 104,            # Displays class probabilities and percentages
           under = TRUE,           # Places text under the nodes
           fallen.leaves = TRUE,   # Positions leaves at the bottom of the tree
           tweak = 1.2,            # Adjusts the size of text
           main = "Pruned Decision Tree (Gini Criterion)")
```

## Key Insights

- **Most Important Variable**: Alcohol content is the primary factor influencing wine quality.
- **Preferred Model**: The pruned decision tree using the Gini index is preferred for its balance between accuracy and simplicity.


## Derive Rules from the Best Model
```{r}
# Extract rules from the pruned Gini tree
library(rpart.plot)
rpart.rules(tree_prune_gini)

# Identify the most important variable
summary(tree_prune_gini)
